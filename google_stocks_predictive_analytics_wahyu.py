# -*- coding: utf-8 -*-
"""Google Stocks Predictive Analytics_Wahyu.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/196a5zyGNjrxm6CZtdk3KHZMjaZTL8CYB

**Import Library Kaggle**

---

Pertama kita import library kaggle untuk mendownload dataset kita
"""

!pip install -q kaggle

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!ls ~/.kaggle

!kaggle datasets download -d akpmpr/google-stock-price-all-time -p /content/sample_data/ --unzip

"""**Import semua library yang dibutuhkan**"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score

from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.neighbors import KNeighborsRegressor

"""**Menampilkan DataFrame**

Menampilkan DataFrame dan mengubah Columns Date menjadi index kita
"""

df = pd.read_csv("/content/sample_data/google.csv", parse_dates=['Date'], index_col='Date')
df

df.info()

"""**Deskripsi Variabel**

* Date : Tanggal perdagangan berlangsung
* Open : Harga pembukaan pada tanggal perdangangan berlangsung
* High : Harga tertinggi pada tanggal perdangangan berlangsung
* Low : Harga terendah pada tanggal perdangangan berlangsung
* Close : Harga terakhir pada saat perdangan pada hari itu di tutup
* Adj Close : Harga penutupan pada hari tersebut setelah disesuaikan
* Volume : Volume transaksi yang terjadi pada tanggal perdagangan berlangsung

**Mengecek apakah ada nilai null pada data yang digunakan**
"""

df.isnull().sum()

high = (df.High == 0).sum()
low = (df.Low == 0).sum()
open = (df.Open == 0).sum()
close = (df.Close == 0).sum()
vol = (df.Volume == 0).sum()
adj = (df['Adj Close'] == 0).sum()


print(high,low,open,close,vol,adj)

"""**Melihat Deskripsi statistika pada dataset yang digunakan**"""

df.describe()

"""# **Visualisasi Data**"""

numerical_cols = ["High", "Low", "Open", "Close", "Volume", "Adj Close"]

plt.subplots(figsize=(10,6))
sns.boxplot(data=df[numerical_cols]).set_title("Google Stock Price")
plt.show()

Q1 = df.quantile(0.35)
Q3 = df.quantile(0.85)
IQR=Q3-Q1
df=df[~((df<(Q1-1.5*IQR))|(df>(Q3+1.5*IQR))).any(axis=1)]
 
# Cek ukuran dataset setelah kita drop outliers
df.shape

plt.subplots(figsize=(10,7))
sns.boxplot(data=df[numerical_cols]).set_title("Google Stock Price")
plt.show()

plt.style.use("ggplot")
plt.figure(figsize=(22,7))
plt.plot(df['High'], label='High Price history',color='g', linestyle="-")
plt.plot(df['Low'], label='Low Price history',color='b', linestyle="--")
plt.plot(df['Open'], label='Open Price history',color='r', linestyle="-.")
plt.plot(df['Close'], label='Close Price history',color='b', linestyle=":")


plt.xlabel('Date',size=15)
plt.ylabel('Stock Price',size=15)
plt.title('Stock Price of Google Over the Years',size=25);
plt.legend()
plt.show()

"""# **Univariate Analysis**

Fitur yang akan diprediksi pada kasus ini terfokus kepada fitur 'Close'
"""

df.hist(bins=50, figsize=(20,15))
plt.show

"""# **Multivariate Analysis**

Dapat kita simpulkan bahwa fitur 'Close' memiliki terkaitan antara fitur 'Open', 'Low', 'High', dan juga 'Adj Close' namun tidak dengan fitur 'Volume'
"""

sns.pairplot(df[numerical_cols], diag_kind='kde')
plt.show()

plt.figure(figsize=(15,8))
corr = df[numerical_cols].corr().round(2)
sns.heatmap(data=corr, annot=True, vmin=-1, vmax=1, cmap='coolwarm', linewidth=1)
plt.title('Correlation matrix for numerical feature', size=15)
plt.show()

df = df.drop(['Volume', 'Adj Close'], axis=1)
df.head()

"""# **Membagi Dataset**"""

x = df.iloc[:, 0:].values
y = df.iloc[:,-1].values
print(x)

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.2, random_state=52)

print(len(x_train))
print(len(x_test))
print(len(y_train))
print(len(y_test))

"""# **Normalisasi Data**

Disini saya menggunakan library MinMaxScaler untuk melakukan normalisasi data, fungsi dari normalisasi data sendiri untuk mempermudah model dalam mempelajari data kita karena data diubah menjadi kebentuk antara 0 sampai dengan 1
"""

scaler = MinMaxScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.fit_transform(x_test)

print(x_train, x_test)

models = pd.DataFrame(columns=['train_mse', 'test_mse'], index=['Gradient Boosting', 'KNN', 'Random Forest'])

"""# **Pembuatan Model**

Disini saya menggunakan Hyperparameter dan GridSearch, penggunaan hyperparameter dan GridSearch Sendiri berguna untuk kita, karena dengan menggunakan hyperparameter dan Gridsearch dapat menentukan parameter model terbaik untuk melakukan pelatihan model kita
"""

def grid_search(model, hyperparameters):
  results = GridSearchCV(
      model,
      hyperparameters,
      cv=5,
      verbose = 1,
      n_jobs = -1,
  )

  return results

gradient_boost = GradientBoostingRegressor()
hyperparameters = {
    'learning_rate': [0.1, 0.01, 0.001, 0.0001, 0.00001],
    'n_estimators': [250, 500, 750, 1000],
    'criterion': ['friedman_mse', 'squared_error'],
}
gradient_boost_search = grid_search(gradient_boost, hyperparameters)
gradient_boost_search.fit(x_train, y_train)
print(gradient_boost_search.best_estimator_)
print(gradient_boost_search.best_params_)
print(gradient_boost_search.best_score_)

dfgb = pd.DataFrame(gradient_boost_search.cv_results_)
dfgb.head()

dfgb[['param_criterion','param_learning_rate', 'param_n_estimators','mean_test_score']]

knn_model = KNeighborsRegressor()
hyperparameters = {
    'n_neighbors': [1,2,3,4,5,6,7,8,9,10],
}

knn_search = grid_search(knn_model, hyperparameters)
knn_search.fit(x_train, y_train)
print(knn_search.best_estimator_)
print(knn_search.best_params_)
print(knn_search.best_score_)

dfknn = pd.DataFrame(knn_search.cv_results_)
dfknn

dfknn[['param_n_neighbors','mean_test_score']]

rdForest = RandomForestRegressor()
hyperparameters = {
    'n_estimators' : [10, 25, 50, 75, 100],
    'criterion' : ['squared_error', 'absolute_error', 'poisson'],
}

rdForestSearch = grid_search(rdForest, hyperparameters)
rdForestSearch.fit(x_train, y_train)
print(rdForestSearch.best_estimator_)
print(rdForestSearch.best_params_)
print(rdForestSearch.best_score_)

dfrdForest = pd.DataFrame(rdForestSearch.cv_results_)
dfrdForest

dfrdForest[['param_criterion','param_n_estimators','mean_test_score','rank_test_score']]

"""# **Model Training**"""

gradient_boost = GradientBoostingRegressor(criterion='friedman_mse', learning_rate=0.01, n_estimators=1000)
gradient_boost.fit(x_train, y_train)

knn = KNeighborsRegressor(n_neighbors=2)
knn.fit(x_train, y_train)

randomForest = RandomForestRegressor(criterion='absolute_error', n_estimators=100)
randomForest.fit(x_train, y_train)

model_dict = {
    'Gradient Boosting':gradient_boost,
    'KNN':knn,
    'Random Forest':randomForest,
}

for name,model in model_dict.items():
  models.loc[name, 'train_mse'] = mean_squared_error(y_train, model.predict(x_train))
  models.loc[name, 'test_mse'] = mean_squared_error(y_test, model.predict(x_test))

models

models.sort_values(by='test_mse', ascending=True).plot(kind="barh", zorder=3)

gradientBoostAcc = gradient_boost.score(x_test, y_test)*100
randomForestAcc = randomForest.score(x_test, y_test)*100
knnAcc = knn.score(x_test, y_test)*100

eval_list = [[gradientBoostAcc],[randomForestAcc],[knnAcc]]
eval = pd.DataFrame(eval_list,
                    columns=['Acc (%)'],
                    index=['Gradient Boosting','Random Forest','K-Nearest Neighbor'])
eval

"""Dari hasil evaluasi di atas dapat kita simpulkan bahwa ketiga model memiliki performa yang hampir menyentuh angka sempurna yaitu 100%, Dimana dapat dilihat juga bahwa model KNN memiliki performa yang paling tinggi yaitu 99.90% dan disusul oleh kedua model lainnya yaitu Random Forest dengan 99.87% dan Gradient Boosting dengan 99.86%"""

prediksi = x_test[:1].copy()
pred_dict = {'y_true':y_test[:1]}
for name, model in model_dict.items():
    pred_dict['prediksi_'+name] = model.predict(prediksi).round(1)
 
pd.DataFrame(pred_dict)

"""Berikut adalah hasil prediksi mendekati nilai asli didadapatkan oleh model KNN"""